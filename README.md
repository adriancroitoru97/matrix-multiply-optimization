# Matrix Multiplication - Homework 2 ASC

Matrix multiplication operation `C=A×B×At+Bt×Bt`,
where A is upper **triangular matrix**, implemented in `C`, using 3 distrinct
methods: `BLAS Atlas` library, unoptimized and optimized.

## Implementations

### BLAS

The operation was divided into steps:
    * As there is no specific data about the B matrix, `cblas_dgemm()` method
      was used to obtain `B^t x B^t`.
      The result of this was stored in a `temp` variable.
    * `cblas_dtrmm()` was used to calculate `A x B`,
      because A is **upper triangular**. As this method stores its result in B,
      the previous step was done before this in order to avoid creating a copy
      of the B matrix.
    * The result of the previous step was multiplied by `A^t` using the general
      `cblas_dgemm()` method, and the result was stored
      in the result variable, `C`.
    * Finally, the `temp(B^t x B^t)` variable is added to
      the result -> `C(A x B x A^t)` using `cblas_daxpy()`, as it adds a vector
      to another vector in constant time.

### Unoptimized

The operation was also divided into steps, just like
we did at the `BLAS` solution. A `temp` matrix is used in calculation.

Because the A matrix is **upper triangular**, when performing the multiplying
only non-null elements are traversed, improving the overall time.

Also, the transposed matrices of A and B are not effectively calculated
but are used inline in the multiplying process by inversing the indexes.

### Optimized

This solution escalated from the **unoptimized implementation**. There were
2 radical optimizations and a few more little ones. Also, the fact that A
matrix is **upper triangular** was considered.

Optimizations ordered descending by importance:
	* **Cache Optimization** -> as it is unefficient to traverse a matrix
	  on columns, the `transposed of B` is computed before the actual
	  multiplying process. This matrix is used in the multiplying process
	  by being traversed on lines (which were the columns of the normal
	  matrix)
	* **Register keyword** -> is used to mark the variables which are most
	  often used in the calculation. This way, the compiler will cache them
	  in order to remove the memory bottleneck generated by bringing them
	  every time from RAM
	* **Pointer Arithmetic** -> direct pointer operations were used in order
	  to make the array accesses faster
	* **Deleting repetitive calculus** -> repetitive operations like `i * N`,
	  for computing the current line, were stored in a upper-level `for` clause
	  in order to avoid re-calculating the same value every time
	* **++i instead of i++** -> all `for` clauses were optimized by replacing
	  the mentioned unary operation, as the `i++` firstly takes a copy of `i`
	* **+ instead of *** -> all additions were replaced by temporar variables,
	  which are only added to each other (because adding is 3x faster)

## Cache analysis & comparison

We can observe that `BLAS` implementation has the *best cache and branch
prediction* performance, with *fewer cache misses and branch mispredictions*
than the other two. This is expected since `BLAS` are highly optimized routines
for linear algebra operations.

The `unoptimized` program performs worse than the first one,
with higher cache miss rates and branch mispredictions. This is due
to the fact that it does not use any optimization and
performs a naive implementation of the matrix multiplication algorithm.

In the `optimized` program we can see that the **number of instruction and data
references** are **similar** to the second program, `tema2_neopt`.
However, the number of L1 and L2 **cache misses are much lower**,
which leads to a much better cache hit rate.
The number of branch instructions and mispredictions are also much lower,
indicating **better branch prediction**.

The results show that the **changing of traversal method** (transposing and
traversing the matrices on lines), using the `register` keyword for key (most
often used) variables and other little optimizations
led to almost **6 times** lower data misses (and refs at all).

## Graph Analysis

![](/grafice/performance.png "")

By far, the `BLAS` implementation performs the best, as it has the lowest
execution time for each value of N. This is due to the use of highly optimized
linear algebra routines provided by the `BLAS library`,
which can significantly accelerate matrix computations.

It can be seen that the `NEOPT` solution is approximately **20 times slower**
than the `BLAS` one.

However, the interesting part is about the `OPT_M` implementation. It is at
most **5 times slower** than `BLAS`, but way quicker than the `NEOPT` one.
These results prove and sustain the importance of **cache optimizations**,
**registering** the most often used variables, and of **micro-optimizations**.

## License

[Adrian-Valeriu Croitoru](https://github.com/adriancroitoru97)
